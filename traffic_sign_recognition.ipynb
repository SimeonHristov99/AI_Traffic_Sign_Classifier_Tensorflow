{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "traffic_sign_recognition.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPBb9LkijxfN/sqpINqN/VK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SimeonHristov99/AI_Traffic_Sign_Classifier_Tensorflow/blob/main/traffic_sign_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9GWMkF2TU1H"
      },
      "source": [
        "# Description\n",
        "\n",
        "## Traffic Sign Classifier\n",
        "\n",
        "### The goal of this project is to build and train a traffic sign classifier. Results were generated in the context of the second  stage  of  the [GTSRB - German Traffic Sign Recognition Benchmark](https://www.kaggle.com/meowmeowmeowmeowmeow/gtsrb-german-traffic-sign) dataset. The classifier is a convolutional neural network and is implemented using the deep learning framework [Keras](https://keras.io/) from the [TensorFlow](https://www.tensorflow.org/) library.\n",
        "\n",
        "### Inspired by:\n",
        "- [CNN Design for Real-Time Traffic Sign Recognition](https://www.sciencedirect.com/science/article/pii/S1877705817341231)\n",
        "- [Man vs. computer: benchmarking machine learning algorithms for traffic sign recognition](https://christian-igel.github.io/paper/MvCBMLAfTSR.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbMdo5NMTtOv"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTdt9MMEWPcF"
      },
      "source": [
        "# Loading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToE-eIenanr-"
      },
      "source": [
        "# # # Run one time only. Requires Kaggle API access key to download the dataset.\n",
        "# !pip install -q kaggle\n",
        "\n",
        "# import os\n",
        "# from google.colab import drive\n",
        "\n",
        "# drive.mount('/gdrive')\n",
        "\n",
        "# assert os.getcwd() == '/content'\n",
        "\n",
        "# os.environ['KAGGLE_CONFIG_DIR'] = '../gdrive/MyDrive/kaggle'\n",
        "\n",
        "# !kaggle datasets download meowmeowmeowmeowmeow/gtsrb-german-traffic-sign -p /content/data/ --unzip"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tg2Fxe4NhRCA"
      },
      "source": [
        "traindf = pd.read_csv('/content/data/Train.csv',dtype=str)"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uB_Odppwki98",
        "outputId": "9363e96e-e94f-4f2c-a3d5-2228e6ec5165"
      },
      "source": [
        "BATCH_SIZE = 50\n",
        "IM_SIZE = 28\n",
        "CLASSES = 43\n",
        "TRAIN_DATA_DIR = '/content/data'\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    # preprocessing_function=tf.keras.applications.vgg16.preprocess_input,\n",
        "    validation_split=0.05, # set validation split tf.image.rgb_to_grayscale,\n",
        "    # rescale=1./255.\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    dataframe=traindf,\n",
        "    directory=TRAIN_DATA_DIR,\n",
        "    x_col='Path',\n",
        "    y_col='ClassId',\n",
        "    seed=42,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    subset='training', # set as training data\n",
        "    target_size=(IM_SIZE, IM_SIZE)\n",
        ")\n",
        "\n",
        "val_generator = train_datagen.flow_from_dataframe(\n",
        "    dataframe=traindf,\n",
        "    directory=TRAIN_DATA_DIR,\n",
        "    x_col='Path',\n",
        "    y_col='ClassId',\n",
        "    seed=42,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    subset='validation', # set as training data\n",
        "    target_size=(IM_SIZE, IM_SIZE)\n",
        ")\n",
        "\n",
        "print(f'{len(train_generator)} training batches. Each with {train_generator[0][0].shape[0]} images.')\n",
        "print(f'{len(val_generator)} validation batches. Each with {val_generator[0][0].shape[0]} images.')"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 37249 validated image filenames belonging to 43 classes.\n",
            "Found 1960 validated image filenames belonging to 43 classes.\n",
            "745 training batches. Each with 50 images.\n",
            "40 validation batches. Each with 50 images.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJV4X9vRT56l"
      },
      "source": [
        "# Visualizing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dx5iSL8zzifO",
        "outputId": "fa6d2095-db51-4da4-99c6-2a552849de09"
      },
      "source": [
        "X_train_batch, y_train_batch = next(train_generator)\n",
        "X_val_batch, y_val_batch = next(val_generator)\n",
        "\n",
        "print(type(X_train_batch), X_train_batch.shape, type(y_train_batch), y_train_batch.shape)\n",
        "print(type(X_val_batch), X_val_batch.shape, type(y_val_batch), y_val_batch.shape)"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'> (50, 28, 28, 3) <class 'numpy.ndarray'> (50, 43)\n",
            "<class 'numpy.ndarray'> (50, 28, 28, 3) <class 'numpy.ndarray'> (50, 43)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "v44C0iURoBFK",
        "outputId": "938d2217-7641-46dd-b49d-e932a2814818"
      },
      "source": [
        "for i in range(10):\n",
        "  plt.subplot(2, 5, i + 1)\n",
        "\n",
        "  rand_idx = np.random.randint(X_train_batch.shape[0])\n",
        "  image = X_train_batch[rand_idx]\n",
        "  \n",
        "  plt.imshow(image)\n",
        "  plt.axis('off')\n",
        "  plt.title(np.argmax(y_train_batch[rand_idx]))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3YlWYswWXeo"
      },
      "source": [
        "# Choosing a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeD4PAFu1rai",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b1aae6b-eb87-4536-aaed-580d19bbb7a6"
      },
      "source": [
        "input_shape = X_train_batch[0].shape\n",
        "\n",
        "# model = keras.Sequential([\n",
        "#  keras.layers.Conv2D(filters=8, kernel_size=3, activation='relu', padding='same', input_shape=input_shape),\n",
        "#  keras.layers.MaxPool2D(),\n",
        "#  keras.layers.Conv2D(filters=16, kernel_size=3, activation='relu', padding='same'),\n",
        "#  keras.layers.MaxPool2D(),\n",
        "#  keras.layers.Flatten(),\n",
        "#  keras.layers.Dropout(0.5),\n",
        "#  keras.layers.Dense(units=CLASSES, activation='softmax')\n",
        "# ])\n",
        "\n",
        "model = keras.models.Sequential([ # lenet_5_model\n",
        "    keras.layers.Conv2D(6, kernel_size=5, strides=1,  activation='relu', input_shape=input_shape, padding='same'), #C1\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.MaxPool2D(), #S2\n",
        "    keras.layers.Dropout(0.2),\n",
        "    keras.layers.Conv2D(16, kernel_size=5, strides=1, activation='relu', padding='same'), #C3\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.MaxPool2D(), #S4\n",
        "    keras.layers.Dropout(0.2),\n",
        "    keras.layers.Flatten(), #Flatten\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(CLASSES, activation='softmax') #Output layer\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_47\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_184 (Conv2D)          (None, 28, 28, 6)         456       \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 28, 28, 6)         24        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_90 (MaxPooling (None, 14, 14, 6)         0         \n",
            "_________________________________________________________________\n",
            "dropout_46 (Dropout)         (None, 14, 14, 6)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_185 (Conv2D)          (None, 14, 14, 16)        2416      \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 14, 14, 16)        64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_91 (MaxPooling (None, 7, 7, 16)          0         \n",
            "_________________________________________________________________\n",
            "dropout_47 (Dropout)         (None, 7, 7, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_47 (Flatten)         (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dropout_48 (Dropout)         (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_87 (Dense)             (None, 43)                33755     \n",
            "=================================================================\n",
            "Total params: 36,715\n",
            "Trainable params: 36,671\n",
            "Non-trainable params: 44\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMEVNNGyWaY3"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaIExiPZ1tpw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba6b7d82-c92c-4cd0-b1b7-c3776fe030d0"
      },
      "source": [
        "model.fit(\n",
        "    x=train_generator,\n",
        "    validation_data=val_generator,\n",
        "    steps_per_epoch = train_generator.samples // BATCH_SIZE,\n",
        "    validation_steps = val_generator.samples // BATCH_SIZE,\n",
        "    epochs = 10\n",
        ")"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "744/744 [==============================] - 13s 17ms/step - loss: 2.4720 - accuracy: 0.4027 - val_loss: 4.6419 - val_accuracy: 0.1508\n",
            "Epoch 2/10\n",
            "744/744 [==============================] - 12s 17ms/step - loss: 0.9022 - accuracy: 0.7282 - val_loss: 4.9908 - val_accuracy: 0.2903\n",
            "Epoch 3/10\n",
            "744/744 [==============================] - 13s 17ms/step - loss: 0.6237 - accuracy: 0.8084 - val_loss: 5.0779 - val_accuracy: 0.5174\n",
            "Epoch 4/10\n",
            "744/744 [==============================] - 12s 17ms/step - loss: 0.5107 - accuracy: 0.8414 - val_loss: 5.4272 - val_accuracy: 0.4662\n",
            "Epoch 5/10\n",
            "744/744 [==============================] - 12s 17ms/step - loss: 0.4582 - accuracy: 0.8585 - val_loss: 6.0550 - val_accuracy: 0.3415\n",
            "Epoch 6/10\n",
            "744/744 [==============================] - 13s 17ms/step - loss: 0.4137 - accuracy: 0.8711 - val_loss: 5.8551 - val_accuracy: 0.4154\n",
            "Epoch 7/10\n",
            "744/744 [==============================] - 13s 17ms/step - loss: 0.3960 - accuracy: 0.8777 - val_loss: 6.1998 - val_accuracy: 0.4651\n",
            "Epoch 8/10\n",
            "744/744 [==============================] - 13s 17ms/step - loss: 0.3742 - accuracy: 0.8853 - val_loss: 6.4437 - val_accuracy: 0.5062\n",
            "Epoch 9/10\n",
            "744/744 [==============================] - 12s 17ms/step - loss: 0.3553 - accuracy: 0.8883 - val_loss: 6.3318 - val_accuracy: 0.4174\n",
            "Epoch 10/10\n",
            "744/744 [==============================] - 13s 17ms/step - loss: 0.3448 - accuracy: 0.8939 - val_loss: 6.1409 - val_accuracy: 0.4985\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f48755b2590>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vsbeq8iLWcRO"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkJKuLMi1uNx"
      },
      "source": [
        "# Coming soon ..."
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qb0xIWq8eV3l"
      },
      "source": [
        ""
      ],
      "execution_count": 174,
      "outputs": []
    }
  ]
}